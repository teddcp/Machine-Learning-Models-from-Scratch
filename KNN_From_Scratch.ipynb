{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN From Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZux34HfeFHYeJBhADDJHs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teddcp/Machine-Learning-Models-from-Scratch/blob/master/KNN_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdKBZ18LUU-",
        "colab_type": "text"
      },
      "source": [
        "# KNN From Scratch\n",
        "-----------------------------------------------\n",
        "\n",
        "This k-Nearest Neighbors tutorial is broken down into 3 parts:\n",
        "```\n",
        "Step 1: Calculate Euclidean Distance.\n",
        "Step 2: Get Nearest Neighbors.\n",
        "Step 3: Make Predictions.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4BtBaUaTdCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sortedcontainers import SortedList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duAhBRMjnITe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class KNN(object):\n",
        "\n",
        "  def __init__(self,k=2) :\n",
        "    self.k=k  \n",
        "    # k is the number of neighbours\n",
        "  \n",
        "  \n",
        "  # Training the model\n",
        "  \n",
        "  def fit(self,X,Y):\n",
        "    # As it is a lazy learner\n",
        "    # During training , it just stores the points\n",
        "    # Thus X and Y are the training points of features and target labels\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "    print('\\n Training is completed... \\n')\n",
        "  \n",
        "\n",
        "  # Calculating the Euclidean Distance between 2 vectors\n",
        "  \n",
        "  def euclid_dist(self,train_x, test_x):\n",
        "    diff= train_x - test_x\n",
        "    val= np.sqrt( diff.dot(diff))\n",
        "    #print(val)\n",
        "    return val\n",
        "  \n",
        "  # Caclulating the testing or training score\n",
        "  # Similar to SKLEARN's score()\n",
        "  \n",
        "  def score(self, X, Y):\n",
        "    pred= self.predict(X)\n",
        "    return np.mean( pred == Y)\n",
        "  \n",
        "  def accuracy_score(self,y_true,y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "  def predict(self,test_x) :\n",
        "\n",
        "    # To Store the result fo predicted labels\n",
        "    pred_y=np.zeros(len(test_x))\n",
        "\n",
        "    for idx1, xt in enumerate(test_x):  # Test points\n",
        "      s1= SortedList()                  # Stores the (distance , Class) in sorted order\n",
        "\n",
        "      \n",
        "      # In this loop, we will detect the k nearest neightbours\n",
        "      # For each testing point with all the training point as its neighburs\n",
        "\n",
        "      for idx2, x in enumerate(self.X): # Training Points\n",
        "        distance_val= self.euclid_dist(x, xt)\n",
        "\n",
        "        if len(s1)< self.k:             \n",
        "          # List is not yet filled with top k nearest neighbours, Thus we can add directly\n",
        "          s1.add( (distance_val, self.Y[idx2]) )\n",
        "        \n",
        "        else:\n",
        "          # List is now filled with k neighbours\n",
        "          # but we are not sure those are the  k nearest or not\n",
        "          # we only need to check the last element's distance of Sorted List\n",
        "          if distance_val < s1[-1][0] :\n",
        "            del s1[-1]\n",
        "            s1.add( (distance_val, self.Y[idx2]) )\n",
        "      \n",
        "\n",
        "      # Now we have detected the k nearest neighbours\n",
        "      # we just need to do a vote to find the most common label\n",
        "      \n",
        "      votes = {}\n",
        "\n",
        "      for _,label in s1 :\n",
        "        votes[label] = votes.get(label,0)+1\n",
        "\n",
        "      # At this moment \n",
        "      # votes will contain the label and its counts\n",
        "      # e.g - votes = { 'class A': 15, 'class B': 20, 'Class C': 30}\n",
        "      \n",
        "      # Now we need to find the maximum number of labels among all the labels\n",
        "      \n",
        "      sorted_votes= sorted( votes.items(), key= lambda x : x[1])\n",
        "      #print(sorted_votes)\n",
        "\n",
        "\n",
        "      # Storing the most common label to the corresponding y\n",
        "      pred_y[idx1]= sorted_votes[-1][0]\n",
        "      #print(pred_y[idx1])\n",
        "      \n",
        "    return pred_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR8J_1V1QO4G",
        "colab_type": "text"
      },
      "source": [
        "Sorted List - http://www.grantjenks.com/docs/sortedcontainers/sortedlist.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbfSlA_2RPjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "x,y= load_iris(return_X_y=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saELlstOZkPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split as tts\n",
        "x_train,x_test,y_train,y_test= tts(x,y,test_size=0.3,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nltuIsqARDcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a73e728-d4c3-40c2-bf36-7d0586eaf8e6"
      },
      "source": [
        "model= KNN(3)\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Training is completed... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP3bUhTWYdig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01248e81-8e60-4ce1-bd32-b1d46297a6fd"
      },
      "source": [
        "res= model.predict(x_test)\n",
        "model.accuracy_score(y_test, res)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU3tNUCBcu5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba790e83-357d-4769-e2db-3b11966cdd1c"
      },
      "source": [
        "model.score(x_test,y_test)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxBHuMDDiZqn",
        "colab_type": "text"
      },
      "source": [
        "## Note\n",
        "-------------------------------\n",
        "\n",
        "1. In the above, we did not implement the ties part. We have assumed there is always going to be a superior class label with maxium vote. For the [ties](https://stats.stackexchange.com/q/144718/279934)\n",
        "\n",
        "2. [Weighted KNN](https://datascience.stackexchange.com/q/64570/94070)\n",
        "\n",
        "3.  Reference for code\n",
        "\n",
        "    **[A](https://github.com/lazyprogrammer/machine_learning_examples/blob/master/supervised_class/knn.py)\n",
        "    **[B](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)\n",
        "\n",
        "    Others:\n",
        "    [1](https://github.com/senavs/knn-from-scratch)  [2](https://dataaspirant.com/2016/12/27/k-nearest-neighbor-algorithm-implementaion-python-scratch/)   [3](https://medium.com/datadriveninvestor/knn-algorithm-and-implementation-from-scratch-b9f9b739c28f)   [4](https://kraj3.com.np/blog/2019/06/implementation-of-knn-from-scratch-in-python/)\n",
        "\n",
        "4. For Regression, we just need to take the mean of top k nearest Neighbours.\n",
        "\n",
        "SKLEARN's [KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
      ]
    }
  ]
}